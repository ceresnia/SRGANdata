{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Boi97bnGIb9w"
      },
      "outputs": [],
      "source": [
        "# # https://youtu.be/nbRkLE2fiVI\n",
        "# https://youtu.be/1HqjPqNglPc\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Dataset from: http://press.liacs.nl/mirflickr/mirdownload.html\n",
        "\n",
        "Read high res. original images and save lower versions to be used for SRGAN.\n",
        "\n",
        "Here, we are resizing them to 128x128 that will be  used as HR images and \n",
        "32x32 that will be used as LR images\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras import layers, Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "from keras import Model\n",
        "from keras.layers import Conv2D, PReLU,BatchNormalization, Flatten\n",
        "from keras.layers import UpSampling2D, LeakyReLU, Dense, Input, add\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "xwuSTYO-Ixck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XKV3sGb6LloO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"/content/drive/MyDrive/SRGANdata\"\n",
        "\n",
        "for img in os.listdir( train_dir + \"/cars_train\"):\n",
        "    img_array = cv2.imread(train_dir + \"/cars_train/\" + img)\n",
        "    \n",
        "    img_array = cv2.resize(img_array, (128,128))\n",
        "    lr_img_array = cv2.resize(img_array,(32,32))\n",
        "\n",
        "    cv2.imwrite(train_dir+ \"/lr_images/\"+ img, lr_img_array)\n",
        "    cv2.imwrite(train_dir+ \"/hr_images/\" + img, img_array)\n",
        "    "
      ],
      "metadata": {
        "id": "Tz6ckiEdI1_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lr_list = os.listdir(train_dir + \"/lr_images\")[:n]"
      ],
      "metadata": {
        "id": "F5iL_JYW8jdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################################\n",
        "\n",
        "#Define blocks to build the generator\n",
        "def res_block(ip):\n",
        "    \n",
        "    res_model = Conv2D(64, (3,3), padding = \"same\")(ip)\n",
        "    res_model = BatchNormalization(momentum = 0.5)(res_model)\n",
        "    res_model = PReLU(shared_axes = [1,2])(res_model)\n",
        "    \n",
        "    res_model = Conv2D(64, (3,3), padding = \"same\")(res_model)\n",
        "    res_model = BatchNormalization(momentum = 0.5)(res_model)\n",
        "    \n",
        "    return add([ip,res_model])\n",
        "\n",
        "def upscale_block(ip):\n",
        "    \n",
        "    up_model = Conv2D(256, (3,3), padding=\"same\")(ip)\n",
        "    up_model = UpSampling2D( size = 2 )(up_model)\n",
        "    up_model = PReLU(shared_axes=[1,2])(up_model)\n",
        "    \n",
        "    return up_model"
      ],
      "metadata": {
        "id": "mhYq56svI4WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generator model\n",
        "def create_gen(gen_ip, num_res_block):\n",
        "    layers = Conv2D(64, (9,9), padding=\"same\")(gen_ip)\n",
        "    layers = PReLU(shared_axes=[1,2])(layers)\n",
        "\n",
        "    temp = layers\n",
        "\n",
        "    for i in range(num_res_block):\n",
        "        layers = res_block(layers)\n",
        "\n",
        "    layers = Conv2D(64, (3,3), padding=\"same\")(layers)\n",
        "    layers = BatchNormalization(momentum=0.5)(layers)\n",
        "    layers = add([layers,temp])\n",
        "\n",
        "    layers = upscale_block(layers)\n",
        "    layers = upscale_block(layers)\n",
        "\n",
        "    op = Conv2D(3, (9,9), padding=\"same\")(layers)\n",
        "\n",
        "    return Model(inputs=gen_ip, outputs=op)"
      ],
      "metadata": {
        "id": "aFloV25pI_Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Descriminator block that will be used to construct the discriminator\n",
        "def discriminator_block(ip, filters, strides=1, bn=True):\n",
        "    \n",
        "    disc_model = Conv2D(filters, (3,3), strides = strides, padding=\"same\")(ip)\n",
        "    \n",
        "    if bn:\n",
        "        disc_model = BatchNormalization( momentum=0.8 )(disc_model)\n",
        "    \n",
        "    disc_model = LeakyReLU( alpha=0.2 )(disc_model)\n",
        "    \n",
        "    return disc_model"
      ],
      "metadata": {
        "id": "5NnNCWEUJAX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Descriminartor, as described in the original paper\n",
        "def create_disc(disc_ip):\n",
        "\n",
        "    df = 64\n",
        "    \n",
        "    d1 = discriminator_block(disc_ip, df, bn=False)\n",
        "    d2 = discriminator_block(d1, df, strides=2)\n",
        "    d3 = discriminator_block(d2, df*2)\n",
        "    d4 = discriminator_block(d3, df*2, strides=2)\n",
        "    d5 = discriminator_block(d4, df*4)\n",
        "    d6 = discriminator_block(d5, df*4, strides=2)\n",
        "    d7 = discriminator_block(d6, df*8)\n",
        "    d8 = discriminator_block(d7, df*8, strides=2)\n",
        "    \n",
        "    d8_5 = Flatten()(d8)\n",
        "    d9 = Dense(df*16)(d8_5)\n",
        "    d10 = LeakyReLU(alpha=0.2)(d9)\n",
        "    validity = Dense(1, activation='sigmoid')(d10)\n",
        "\n",
        "    return Model(disc_ip, validity)"
      ],
      "metadata": {
        "id": "TOjOVAnpJC_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#VGG19 \n",
        "#We need VGG19 for the feature map obtained by the j-th convolution (after activation) \n",
        "#before the i-th maxpooling layer within the VGG19 network.(as described in the paper)\n",
        "#Let us pick the 3rd block, last conv layer. \n",
        "#Build a pre-trained VGG19 model that outputs image features extracted at the\n",
        "# third block of the model\n",
        "# VGG architecture: https://github.com/keras-team/keras/blob/master/keras/applications/vgg19.py\n",
        "from keras.applications import VGG19\n",
        "\n",
        "def build_vgg(hr_shape):\n",
        "    \n",
        "    vgg = VGG19(weights=\"imagenet\",include_top=False, input_shape=hr_shape)\n",
        "    \n",
        "    return Model(inputs=vgg.inputs, outputs=vgg.layers[10].output)\n"
      ],
      "metadata": {
        "id": "uTA8vnRIJHxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combined model\n",
        "def create_comb(gen_model, disc_model, vgg, lr_ip, hr_ip):\n",
        "    gen_img = gen_model(lr_ip)\n",
        "    \n",
        "    gen_features = vgg(gen_img)\n",
        "    \n",
        "    disc_model.trainable = False\n",
        "    validity = disc_model(gen_img)\n",
        "    \n",
        "    return Model(inputs=[lr_ip, hr_ip], outputs=[validity, gen_features])\n",
        "\n",
        "# 2 losses... adversarial loss and content (VGG) loss\n",
        "#AdversariaL: is defined based on the probabilities of the discriminator over all training samples\n",
        "# use binary_crossentropy\n",
        "\n",
        "#Content: feature map obtained by the j-th convolution (after activation) \n",
        "#before the i-th maxpooling layer within the VGG19 network.\n",
        "# MSE between the feature representations of a reconstructed image\n",
        "# and the reference image. "
      ],
      "metadata": {
        "id": "FMeN8ssFJLDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load first n number of images (to train on a subset of all images)\n",
        "#For demo purposes, let us use 5000 images\n",
        "n=5000\n",
        "lr_list = os.listdir(train_dir + \"/lr_images\")[:n]\n",
        "\n",
        "lr_images = []\n",
        "for img in lr_list:\n",
        "    img_lr = cv2.imread(train_dir + \"/lr_images/\" + img)\n",
        "    img_lr = cv2.cvtColor(img_lr, cv2.COLOR_BGR2RGB)\n",
        "    lr_images.append(img_lr)   \n",
        "\n",
        "\n",
        "hr_list = os.listdir(train_dir + \"/hr_images\")[:n]\n",
        "   \n",
        "hr_images = []\n",
        "for img in hr_list:\n",
        "    img_hr = cv2.imread(train_dir + \"/hr_images/\" + img)\n",
        "    img_hr = cv2.cvtColor(img_hr, cv2.COLOR_BGR2RGB)\n",
        "    hr_images.append(img_hr)   \n",
        "\n",
        "lr_images = np.array(lr_images)\n",
        "hr_images = np.array(hr_images)"
      ],
      "metadata": {
        "id": "ukTE6JlmJOL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sanity check, view few mages\n",
        "import random\n",
        "import numpy as np\n",
        "image_number = random.randint(0, len(lr_images)-1)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(121)\n",
        "plt.imshow(np.reshape(lr_images[image_number], (32, 32, 3)))\n",
        "plt.subplot(122)\n",
        "plt.imshow(np.reshape(hr_images[image_number], (128, 128, 3)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r05_KnRFJU9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scale values\n",
        "lr_images = lr_images / 255.\n",
        "hr_images = hr_images / 255.\n",
        "\n",
        "#Split to train and test\n",
        "lr_train, lr_test, hr_train, hr_test = train_test_split(lr_images, hr_images, \n",
        "                                                      test_size=0.33, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "hr_shape = (hr_train.shape[1], hr_train.shape[2], hr_train.shape[3])\n",
        "lr_shape = (lr_train.shape[1], lr_train.shape[2], lr_train.shape[3])\n",
        "\n",
        "lr_ip = Input(shape=lr_shape)\n",
        "hr_ip = Input(shape=hr_shape)\n",
        "\n",
        "generator = create_gen(lr_ip, num_res_block = 16)\n",
        "generator.summary()"
      ],
      "metadata": {
        "id": "u5imXzhHJfv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = create_disc(hr_ip)\n",
        "discriminator.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "discriminator.summary()\n",
        "\n",
        "vgg = build_vgg((128,128,3))\n",
        "print(vgg.summary())\n",
        "vgg.trainable = False"
      ],
      "metadata": {
        "id": "C-_H607uJdH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gan_model = create_comb(generator, discriminator, vgg, lr_ip, hr_ip)\n",
        "\n",
        "# 2 losses... adversarial loss and content (VGG) loss\n",
        "#AdversariaL: is defined based on the probabilities of the discriminator over all training samples\n",
        "# use binary_crossentropy\n",
        "\n",
        "#Content: feature map obtained by the j-th convolution (after activation) \n",
        "#before the i-th maxpooling layer within the VGG19 network.\n",
        "# MSE between the feature representations of a reconstructed image\n",
        "# and the reference image. \n",
        "gan_model.compile(loss=[\"binary_crossentropy\", \"mse\"], loss_weights=[1e-3, 1], optimizer=\"adam\")\n",
        "gan_model.summary()"
      ],
      "metadata": {
        "id": "t7DfvcKOJa0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a list of images for LR and HR in batches from which a batch of images\n",
        "#would be fetched during training. \n",
        "batch_size = 1  \n",
        "train_lr_batches = []\n",
        "train_hr_batches = []\n",
        "for it in range(int(hr_train.shape[0] / batch_size)):\n",
        "    start_idx = it * batch_size\n",
        "    end_idx = start_idx + batch_size\n",
        "    train_hr_batches.append(hr_train[start_idx:end_idx])\n",
        "    train_lr_batches.append(lr_train[start_idx:end_idx])"
      ],
      "metadata": {
        "id": "kDXeZpQlJYCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "#Enumerate training over epochs\n",
        "for e in range(epochs):\n",
        "    \n",
        "    fake_label = np.zeros((batch_size, 1)) # Assign a label of 0 to all fake (generated images)\n",
        "    real_label = np.ones((batch_size,1)) # Assign a label of 1 to all real images.\n",
        "    \n",
        "    #Create empty lists to populate gen and disc losses. \n",
        "    g_losses = []\n",
        "    d_losses = []\n",
        "    \n",
        "    #Enumerate training over batches. \n",
        "    for b in tqdm(range(len(train_hr_batches))):\n",
        "        lr_imgs = train_lr_batches[b] #Fetch a batch of LR images for training\n",
        "        hr_imgs = train_hr_batches[b] #Fetch a batch of HR images for training\n",
        "        \n",
        "        fake_imgs = generator.predict_on_batch(lr_imgs) #Fake images\n",
        "        \n",
        "        #First, train the discriminator on fake and real HR images. \n",
        "        discriminator.trainable = True\n",
        "        d_loss_gen = discriminator.train_on_batch(fake_imgs, fake_label)\n",
        "        d_loss_real = discriminator.train_on_batch(hr_imgs, real_label)\n",
        "        \n",
        "        #Now, train the generator by fixing discriminator as non-trainable\n",
        "        discriminator.trainable = False\n",
        "        \n",
        "        #Average the discriminator loss, just for reporting purposes. \n",
        "        d_loss = 0.5 * np.add(d_loss_gen, d_loss_real) \n",
        "        \n",
        "        #Extract VGG features, to be used towards calculating loss\n",
        "        image_features = vgg.predict(hr_imgs)\n",
        "     \n",
        "        #Train the generator via GAN. \n",
        "        #Remember that we have 2 losses, adversarial loss and content (VGG) loss\n",
        "        g_loss, _, _ = gan_model.train_on_batch([lr_imgs, hr_imgs], [real_label, image_features])\n",
        "        \n",
        "        #Save losses to a list so we can average and report. \n",
        "        d_losses.append(d_loss)\n",
        "        g_losses.append(g_loss)\n",
        "        \n",
        "    #Convert the list of losses to an array to make it easy to average    \n",
        "    g_losses = np.array(g_losses)\n",
        "    d_losses = np.array(d_losses)\n",
        "    \n",
        "    #Calculate the average losses for generator and discriminator\n",
        "    g_loss = np.sum(g_losses, axis=0) / len(g_losses)\n",
        "    d_loss = np.sum(d_losses, axis=0) / len(d_losses)\n",
        "    \n",
        "    #Report the progress during training. \n",
        "    print(\"epoch:\", e+1 ,\"g_loss:\", g_loss, \"d_loss:\", d_loss)\n",
        "\n",
        "    if (e+1) % 10 == 0: #Change the frequency for model saving, if needed\n",
        "        #Save the generator after every n epochs (Usually 10 epochs)\n",
        "        generator.save(\"gen_e_\"+ str(e+1) +\".h5\")\n"
      ],
      "metadata": {
        "id": "tcNmMw7HJlrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################################\n",
        "#Test - perform super resolution using saved generator model\n",
        "from keras.models import load_model\n",
        "from numpy.random import randint\n",
        "\n",
        "generator = load_model('gen_e_10.h5', compile=False)\n",
        "\n",
        "\n",
        "[X1, X2] = [lr_test, hr_test]\n",
        "# select random example\n",
        "ix = randint(0, len(X1), 1)\n",
        "src_image, tar_image = X1[ix], X2[ix]\n",
        "\n",
        "# generate image from source\n",
        "gen_image = generator.predict(src_image)\n",
        "\n",
        "\n",
        "# plot all three images\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(231)\n",
        "plt.title('LR Image')\n",
        "plt.imshow(src_image[0,:,:,:])\n",
        "plt.subplot(232)\n",
        "plt.title('Superresolution')\n",
        "plt.imshow(gen_image[0,:,:,:])\n",
        "plt.subplot(233)\n",
        "plt.title('Orig. HR image')\n",
        "plt.imshow(tar_image[0,:,:,:])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fCRDnibfJppZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################################################\n",
        "sreeni_lr = cv2.imread(\"data/sreeni_32.jpg\")\n",
        "sreeni_hr = cv2.imread(\"data/sreeni_256.jpg\")\n",
        "\n",
        "#Change images from BGR to RGB for plotting. \n",
        "#Remember that we used cv2 to load images which loads as BGR.\n",
        "sreeni_lr = cv2.cvtColor(sreeni_lr, cv2.COLOR_BGR2RGB)\n",
        "sreeni_hr = cv2.cvtColor(sreeni_hr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "sreeni_lr = sreeni_lr / 255.\n",
        "sreeni_hr = sreeni_hr / 255.\n",
        "\n",
        "sreeni_lr = np.expand_dims(sreeni_lr, axis=0)\n",
        "sreeni_hr = np.expand_dims(sreeni_hr, axis=0)\n",
        "\n",
        "generated_sreeni_hr = generator.predict(sreeni_lr)\n",
        "\n",
        "# plot all three images\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(231)\n",
        "plt.title('LR Image')\n",
        "plt.imshow(sreeni_lr[0,:,:,:])\n",
        "plt.subplot(232)\n",
        "plt.title('Superresolution')\n",
        "plt.imshow(generated_sreeni_hr[0,:,:,:])\n",
        "plt.subplot(233)\n",
        "plt.title('Orig. HR image')\n",
        "plt.imshow(sreeni_hr[0,:,:,:])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V0QLkMHgInY5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}